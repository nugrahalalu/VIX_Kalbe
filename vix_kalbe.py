# -*- coding: utf-8 -*-
"""VIX_Kalbe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zmm4cICFEDlsSLDHJpt-fUdX0VikLZ4C

## Library Import
"""

import pandas as pd
import numpy as np

# Visualization Library
import matplotlib.pyplot as plt
import seaborn as sns

# Ignore Warnings
import warnings
warnings.filterwarnings("ignore")

"""## Read The Datasets"""

from google.colab import drive
drive.mount('/content/drive')

cust  = pd.read_csv ("/content/drive/MyDrive/VIX/Kalbe/Case Study - Customer.csv", delimiter = ";")
prod  = pd.read_csv ("/content/drive/MyDrive/VIX/Kalbe/Case Study - Product.csv", delimiter = ";")
store = pd.read_csv ("/content/drive/MyDrive/VIX/Kalbe/Case Study - Store.csv", delimiter = ";")
trans = pd.read_csv ("/content/drive/MyDrive/VIX/Kalbe/Case Study - Transaction.csv", delimiter = ";")

"""## Dataset Dictionary

This dataset consists of 4 CSV files: customer, store, product, and transaction. It is dummy data for an FMCG case study over a span of 1 year, collected through a membership program.

| Table Name | Column Name | Description |
|------|---------|---------|
| **Customer** | CustomerID | Unique Identifier for each Customer |
|  | Gender | 0 : Female</br> 1 : Male |
|  | Marital Status | Married </br> Single (Divorced or Not yet Married |
|  | Income | Income per Month (Million IDR) |
|  |  |  |  |
| **Store** | StoreID | Unique Identifier for each Store |
|  | StoreName | Store Name |
|  | GroupStore | Store Group |
|  | Type | Modern Trade</br>General Trade |
|  | Latitude | Latitude Code |
|  | Longitude | Longitude Code |
|  |  |  |  |
| **Product** | ProductID | Unique Identifier for each Product |
|  | Product Name | Product Name |
|  | Price | Price of Product (IDR) |
|  |  |  |  |
| **Transaction** | TransactionID | Unique Identifier for each Transaction |
|  | Date | Date of Transaction |
|  | Qty | The quantity of products purchased. |
|  | TotalAmount | Revenue (Price x Qty) |

## Brief Overview of Each Dataset
"""

cust.head()

prod.head()

store.head()

trans.head()



"""### Note:
Exploratory Data Analysis (EDA) was conducted using DBeaver. You can review the analysis in the `.psql` file.
"""



"""## Data Cleaning

### Check Missing Values, Duplicated Values, and Data Type
"""

datasets = {
     "Customer" : cust,
     "Product"  : prod,
     "Store"    : store,
     "Transaction" : trans }

def data_check (datasets):
    results = {}

    for name, dataset in datasets.items():

        # Check for missing values
        missing_values = dataset.isnull().sum()
        missing_percentage = round (missing_values/len(dataset)*100, 2)
        data_type = dataset.dtypes
        missing_data = pd.concat ([missing_values, missing_percentage, data_type],
                                 keys = ["missing_count", "missing_percentage", "data_type"],
                                 axis = 1)

        # Check for duplicate rows
        duplicate_rows = dataset.duplicated().sum()

        # Check Data Shape
        data_shape = dataset.shape

        # Check Nunique Values for each ID
        ids = dataset.columns[0]
        nunique_values = dataset[ids].nunique()

        results[name] = {
            "missing_values": missing_data,
            "duplicate_rows": duplicate_rows,
            "data_shape" : data_shape,
            "ids" : ids,
            "nunique_ids" : nunique_values}
    return results

results = data_check (datasets)

for name, result in results.items():
    print(f"Dataset: {name}")
    print("Missing Values:")
    print (result["missing_values"])
    print("\nDuplicate Rows:")
    print(result["duplicate_rows"])
    print(f"\nData Shape : {result ['data_shape']}")
    print(f"Count of Unique Values of {result ['ids']} : {result['nunique_ids']}")
    print("=" * 60, "\n")

"""From the above result, we can see that:
</br>

**1. Customer dataset:**
- has missing values in Marital Status column with count of 3
- has no duplicated values
- data type for Income column is Object. This data type should be changed to Float to make it more suitable data type
- The data shapes and the count of unique ID values match.

**2. Product dataset:**
- has no missing and duplicated values
- data type for each column is good
- The data shapes and the count of unique ID values match.

**3. Store dataset:**
- has no missing and duplicated values
- data type for Latitude and Longitude columns are Object. These data types should be changed to Float to make it more suitable data type
- The data shapes and the count of unique ID values match.

**4. Transaction dataset:**
- has no missing and duplicated values
- data type for Date column is Object. This data type should be changed to Date time to make it more suitable data type
- The data shapes and the count of unique ID values do not match. This indicates duplicated values in the TransactionID column
"""



"""#### 1. Customer Dataset : Income Column

##### Handling Missing Values
"""

# Fill the missing values using the mode

cust ["Marital Status"].fillna (cust ["Marital Status"].mode()[0], inplace = True)

# Check the result

cust.isnull().sum().sum()

"""##### Change Data Type From Object to Float"""

cust ["Income"] = cust ["Income"].apply(lambda x: x.replace(",", ".")).astype (float)

cust.info()

"""#### 2. Store Dataset : Latitude and Longitude Columns

##### Change Data Type From Object to Float
"""

store ["Longitude"] = store ["Longitude"].apply(lambda x: x.replace(",", ".")).astype(float)
store ["Latitude"] = store ["Latitude"].apply(lambda x: x.replace(",", ".")).astype(float)

store.info()

"""#### 3. Transaction Dataset : Date Column

##### Change Data Type From Object to Datetime
"""

trans ["Date"] = pd.to_datetime (trans ["Date"], dayfirst = True)

# Check the format of date column

date_col = trans ["Date"]
format_date = "%d/%m/%y"

try:
    parsed_date = pd.to_datetime(date_col, format = format_date)
    print("Date string matches the desired format.")
    print("Parsed date:\n", parsed_date)
except ValueError:
    print("Date string does not match the desired format.")

"""##### Drop Duplicated Values
Duplicated Values in the TransactionID will be handled after merging all of the datasets. This will help us to identify the duplicated values with more comprehensive information from all of the datasets.

## Merge Datasets
"""

df1 = pd.merge(cust, trans, on = "CustomerID", how = "inner")
df2 = pd.merge(df1, prod, on = "ProductID", how = "inner")
df_merged = pd.merge(df2, store, on = "StoreID", how = "inner")

df_merged.head()

# Price_y and Price_x have the same values, so we will drop one of them

df_merged.drop (columns = ("Price_y"), inplace = True)

df_merged.rename (columns = {"Price_x": "Price"}, inplace = True)

"""### Check Unique Values from TransactionID"""

# Check transactionID
print ("total rows data :", df_merged.shape [0])
print ("total unique TransactionID :", df_merged ["TransactionID"].nunique())
print ("duplicated TransactionID :", df_merged ["TransactionID"].duplicated().sum())

"""TransactionID should be unique for each transaction, but as the above result shows that there 112 duplicated TransactionID"""

# Check the duplicated Transaction ID

dup_transID = df_merged [df_merged ["TransactionID"].duplicated (keep = False)]
dup_count = dup_transID.groupby ("TransactionID").size()
dup_count.unique()

# Find the TransactionID with more than 2 duplicates

dup_more = dup_count [dup_count > 2]
dup_more

# Check the TransactionID

df_merged [df_merged ["TransactionID"] == "TR71313"]

"""The result shows duplicated TransactionIDs with different CustomerIDs. TransactionID should be unique so it can be used to trace each transaction. It's not expected to have the same TransactionID associated with different CustomerIDs, especially when there are discrepancies in the transaction dates. This kind of situation indicates underlying data or process issues.</br>"""

# Check The Other TransactionIDs using same CustomerID and ProductID

df_merged [(df_merged ["CustomerID"] == 401) & (df_merged ["ProductID"] == "P5")]

"""TransactionID should be like this. Each transaction has a different TransactionID even if it has the same CustomerID, ProductID, and StoreID. But there's a difference in other information like the Date of the Transaction is made.

Based on these findings, the data rows with duplicated TransactionIDs will be removed, keeping only one row with the earliest transaction time according to the date column.

### Drop Duplicated TransactionID
"""

# Sort the data based on the Date column
df_merged.sort_values ("Date", inplace = True)

# Drop duplicated transactionID and only keep the earliest transactionID based on Date column
df_merged.drop_duplicates (subset = "TransactionID", keep = "first", inplace = True)

# Reset the index of the dataset
df_merged.reset_index(drop = True, inplace = True)
df_merged.head()

df_merged.shape



"""# <center>Data Preparation<center>

## 1. Group By For Time Series Dataset
"""

df_ts = df_merged.groupby ("Date").agg ({"Qty" : "sum"}).reset_index()
df_ts.head(13)

"""The date format of df_merged ["Date"] is `"yyyy/mm/dd"` and by default it's sorted by **month** value of Date column

### **Comparison**
"""

# Import the original dataset of transaction

transc_ori = pd.read_csv ("/content/drive/MyDrive/VIX/Kalbe/Case Study - Transaction.csv", sep = ";")

# Group by date

transc_ori = transc_ori.groupby ("Date").agg ({"Qty" : "sum"}).reset_index()
transc_ori.head(13)

"""The format date of Date column from the original dataset of transaction is `"dd/mm/yyyy"` and by default it's sorted by **day** value of Date column. This is why the result does not match with df_ts group by"""

# Check The result based on the first 5 days of january from the original dataset of transaction
# This is to match the result with groupby Date from df_ts

print (transc_ori [transc_ori ["Date"] == "01/01/2022"])
print (transc_ori [transc_ori ["Date"] == "02/01/2022"])
print (transc_ori [transc_ori ["Date"] == "03/01/2022"])
print (transc_ori [transc_ori ["Date"] == "04/01/2022"])
print (transc_ori [transc_ori ["Date"] == "05/01/2022"])

"""From the above result between groupby **Date** of :
- df_ts that we've converted and
- the first 5 days of the original dataset of transaction

we can see that the result of total quantity is the same. This means that we have successfully converted the Date column the right way.
"""

# We need to set date column as the index

df_ts = df_ts.set_index ("Date")

df_ts.plot(figsize = (16,4))
plt.show()

# Check for the trend seasonality and residual
from statsmodels.tsa.seasonal import seasonal_decompose

decompose = seasonal_decompose(df_ts)

fig,ax = plt.subplots(3,1,figsize=(15,12))
decompose.trend.plot(ax=ax[0])
ax[0].set_title('Trend')
decompose.seasonal.plot(ax=ax[1])
ax[1].set_title('Seasonal')
decompose.resid.plot(ax=ax[2])
ax[2].set_title('Residual')

plt.tight_layout()
plt.show()



"""## 2. Stationarity Test

Stationarity is an important concept in time series analysis because many time series models like ARIMA assume that the data is stationary. To check the stationarity we use 2 methods, first method is using rolling mean and rolling std and visualize it using plot, second method is using The Augmented Dickey-Fuller or ADF-Test.
"""

def stationarity_test (dataset, col):
    # Rolling mean and std:
    roll_mean = dataset[col].rolling(window=12).mean()
    roll_std = dataset[col].rolling (window = 12).std()

    # Visualize:
    plt.figure (figsize = (15,5))

    plt.plot(dataset, label= "Original")
    plt.plot(roll_mean, label= "Rolling Mean")
    plt.plot(roll_std, label= "Rolling Std")
    plt.legend(loc='best')
    plt.title("Stationarity Test by Rolling Mean & Rolling Standard Deviation")
    plt.show()

    # Augmented Dickey–Fuller (ADF - test)
    from statsmodels.tsa.stattools import adfuller
    print ("Stationarity Test by ADF - Test :\n")
    adf_test = adfuller(dataset[col], autolag = "AIC")
    output = pd.Series(adf_test[0:4], index=["Test Statistic :", "p-value :", "Number of Lags Used :", "Number of Observations Used :"])

    for key,value in adf_test[4].items():
        output ["Critical Value (%s) :" %key] = value
    print (output)

stationarity_test (df_ts, "Qty")

"""From the stationarity_test, we can see that the df_ts is stationary based on p-value which is less than significance level (0.0000 < 0.05)

## 3. ACF and PACF
"""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

fig, ax = plt.subplots(1, 2, figsize=(15, 5))
plot_acf(df_ts, ax=ax[0])
ax[0].grid()
plot_pacf(df_ts, ax=ax[1])
ax[1].grid()
plt.show()



fig, ax = plt.subplots(1, 2, figsize=(25, 5))
plot_acf(df_ts.diff().dropna(), ax=ax[0])
ax[0].grid()
plot_pacf(df_ts.diff().dropna(), ax=ax[1])
ax[1].grid()
plt.show()

"""## 4. Split Data Train and Data Test"""

# 80% for data train
# 20% for data test

print("dataset shape :", df_ts.shape)

test_size = round(df_ts.shape[0] * 0.2)
df_train=df_ts.iloc[:-1*(test_size)]
df_test=df_ts.iloc[-1*(test_size):]

print("data train shape:", df_train.shape)
print ("data test shape :", df_test.shape)

plt.figure (figsize = (15,5))
plt.plot(df_train, color = "black")
plt.plot(df_test, color = "red")
plt.ylabel("Quanity")
plt.xlabel("Date")
plt.title("Data Split")
plt.show()

"""From the above results, we can see that the values in the training data (df_train) continue into the testing data (df_test). This indicates that we have successfully split the data as intended."""



"""# <center>Time Series Modelling<center>

## 1. ARIMA Model
"""

!pip install pmdarima

from statsmodels.tsa.arima.model import ARIMA
import pmdarima as pm
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error



"""### 1. Auto ARIMA"""

auto_model = pm.auto_arima (df_train ["Qty"], stepwise = False, seasonal = True)
auto_model

"""### 2. Manual ARIMA"""

model = ARIMA (df_train, order = [2, 0, 2])
arima_model = model.fit()

"""### Model Training"""

hyper_forecast = arima_model.forecast(len(df_test))
auto_forecast = auto_model.predict(len(df_test))

df_plot = df_ts.iloc[-73:]

df_plot['hyper_forecast'] = [None]*(len(df_plot)-len(hyper_forecast)) + list(hyper_forecast)
df_plot['auto_forecast'] = [None]*(len(df_plot)-len(auto_forecast)) + list(auto_forecast)

df_plot.plot()
plt.show()

"""### Forecasting"""

model = ARIMA(df_ts, order=(5, 1, 0))
model_fit = model.fit()
forecast = model_fit.forecast(steps=31)

"""### Visualize Forecasting"""

plt.figure(figsize=(12,5))
plt.plot(df_ts)
plt.plot(forecast,color='red')
plt.title('Quantity Sales Forecasting')
plt.show()

forecast.describe()

"""From the above result we can see that the average of quantity forecasting using Auto ARIMA order is 47 pcs/day. But the result of forecasting is not good enough since the graph shows flat line in our forecast of january 2023. The next step we will try to use SARIMAX.

## 2. SARIMAX Model
"""

df_sar = df_ts.copy()

# Check for the trend seasonality and residual
from statsmodels.tsa.seasonal import seasonal_decompose

decompose = seasonal_decompose(df_sar)

fig,ax = plt.subplots(3,1,figsize=(15,12))
decompose.trend.plot(ax=ax[0])
ax[0].set_title('Trend')
decompose.seasonal.plot(ax=ax[1])
ax[1].set_title('Seasonal')
decompose.resid.plot(ax=ax[2])
ax[2].set_title('Residual')

plt.tight_layout()
plt.show()

"""### using .diff(7)"""

df_sar ["Diff7"] = df_sar.diff(7)

from statsmodels.tsa.stattools import adfuller
print ("Stationarity Test by ADF - Test :\n")
adf_test = adfuller(df_sar ["Diff7"].dropna(), autolag = "AIC")
output = pd.Series(adf_test[0:4], index=["Test Statistic :", "p-value :", "Number of Lags Used :", "Number of Observations Used :"])

for key,value in adf_test[4].items():
    output ["Critical Value (%s) :" %key] = value
print (output)

"""We can see from the above result that df_sar p-value is less than significant level (2.09 -12 < 0.05). This means that df_sar for quantity after differencing by 7 is stationare

### ACF and PACF

#### 1. Original Dataset
"""

fig, ax = plt.subplots(1, 2, figsize=(15, 5))
plot_acf(df_sar["Qty"], ax=ax[0])
ax[0].grid()
plot_pacf(df_sar["Qty"], ax=ax[1])
ax[1].grid()
plt.show()

"""From the above result we can see that the ARIMA is (0,0,0) because it.s the original data and we don't use any differencing and there is no significant value from acf and pacf graph

#### 2. Diff7
"""

fig, ax = plt.subplots(1, 2, figsize=(15, 5))
plot_acf(df_sar ["Diff7"].dropna(), ax=ax[0])
ax[0].grid()
plot_pacf(df_sar ["Diff7"].dropna(), ax=ax[1])
ax[1].grid()
plt.show()

"""from the above result, we can see that weather acf or pacf shows the significant value is at lags of 7 and the multiple of it. The presence of significant spikes at these lags suggests a seasonal pattern with a seasonal period of 7 (weekly."""

import statsmodels.api as sm

model = sm.tsa.SARIMAX (df_test, order = (0,0,0), seasonal_order = (1,1,0,7))

model_fit = model.fit()
model_fit.summary()

forecast = model_fit.get_forecast(steps = 31)
forecast_result = forecast.conf_int()
forecast_result ["Forecast Qty"] = model_fit.predict (start= forecast_result.index [0],
                                                      end = forecast_result.index [-1])
forecast_result ["Date"] = pd.date_range (start = "2023-01-01", end = "2023-01-31")
forecast_result.set_index ("Date", inplace = True)
forecast_result.head()

plt.figure (figsize = (15,5))
plt.plot (df_train)
plt.plot (df_test)
plt.plot (forecast_result ["Forecast Qty"])

forecast_result.describe()

"""We can see that the average forecast from SARIMAX is also 47 pcs/day just like in ARIMA but with better overall result

# <center>Clustering<center>
"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_samples, silhouette_score
from itertools import permutations
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

df_merged2 = df_merged.copy()

df_cluster = df_merged2.groupby('CustomerID').agg({'TransactionID':'count','Qty':'sum','TotalAmount':'sum'})
df_cluster.head()

#Check outliers on new dataset
features = df_cluster.columns
fig, ax = plt.subplots(1,len(df_cluster.columns),figsize=(12,5))
for i in range(0,len(df_cluster.columns)):
    sns.boxplot(data=df_cluster,y=features[i],ax=ax[i])
plt.tight_layout()
plt.show()

"""There's some outliers but the data is still good, no need to handle outliers right now"""

# transformation
X = df_cluster.values
X_ss = StandardScaler().fit_transform(X)
df_ss = pd.DataFrame(data=X_ss,columns=df_cluster.columns)

df_ss.head()

"""# Model Clustering

## Elbow Method
"""

#Elbow method
inertia = []

for i in range(1,11):
    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)
    kmeans.fit(df_ss.values)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(10,6))
plt.xticks(list(range(1,11)))
plt.plot(range(1,11),inertia,marker='o')
plt.title('Elbow Method')
plt.show()



"""Elbow method the optimal cluster is when n = 3"""



# Kmeans n_cluster = 3

kmeans_3 = KMeans(n_clusters = 3, init = "k-means++", max_iter = 300, n_init = 10, random_state = 0)
kmeans_3.fit(X_ss)

df_clust3 = pd.DataFrame(data=X_ss,columns=df_cluster.columns)
df_clust3['Cluster'] = kmeans_3.labels_
df_clust3.head(3)

#PLot Before PCA
plt.figure(figsize=(4,4))
sns.pairplot(data=df_clust3, hue= "Cluster",palette='Set1')
plt.show()

"""We can see from the above plot, the cluster has separated well

## PCA
"""

#PCA
pc_3 = PCA(n_components=2).fit_transform(X_ss)
df_pc3 = pd.DataFrame(data=pc_3,columns=["pc_1", "pc_2"])
df_pc3['Cluster'] = df_clust3 ["Cluster"]
df_pc3.describe().T

"""### PCA Visualiziation"""

#PCA plot

fig,ax = plt.subplots(2,1,figsize=(10,8))

sns.scatterplot(data = df_pc3, x= "pc_1", y = "pc_2", hue = "Cluster", palette = "Set1", ax = ax[0])
ax[0].set_title("PCA")

sns.kdeplot(data = df_pc3, x= "pc_1" ,hue = "Cluster", palette = "Set1", fill = True, ax = ax[1])
ax[1].set_title('PCA kde')
plt.tight_layout()
plt.show()

"""## Sillhouette analysis"""

n_clust = list (range (2,11))

avg_silhouette = []
for i in n_clust:
    kmeans = KMeans(n_clusters = i, init = "k-means++", max_iter = 300, n_init = 10, random_state = 0)
    labels = kmeans.fit_predict(X_ss)
    avg_silhouette.append(silhouette_score (X_ss, labels))

plt.title("Silhouette analysis for optimal n_clusters")
plt.plot(n_clust, avg_silhouette)
plt.xlabel("n_clusters")
plt.ylabel("Silhouette score")
plt.show()

from matplotlib import cm

range_n_clusters = [2,3, 4]
X = X_ss
for n_clusters in range_n_clusters:
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    ax1.set_xlim([-0.1, 1])
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    clusterer = KMeans(n_clusters=n_clusters, init='k-means++',max_iter=300,n_init=10,random_state=0)
    cluster_labels = clusterer.fit_predict(X)

    avg_silhouette = silhouette_score(X, cluster_labels)
    print(
        "For n_clusters =",
        n_clusters,
        "The average silhouette_score is :",
        avg_silhouette)
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()
        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(
            np.arange(y_lower, y_upper),
            0,
            ith_cluster_silhouette_values,
            facecolor=color,
            edgecolor=color,
            alpha=0.7)

        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    ax1.axvline(x=avg_silhouette, color="red", linestyle="--")
    ax1.set_yticks([])
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(
        X[:, 0], X[:, 1], marker=".", s=30, lw=0, alpha=0.7, c=colors, edgecolor="k")
    centers = clusterer.cluster_centers_
    ax2.scatter(
        centers[:, 0],
        centers[:, 1],
        marker="o",
        c="white",
        alpha=1,
        s=200,
        edgecolor="k")

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker="$%d$" % i, alpha=1, s=50, edgecolor="k")

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle("Silhouette analysis for KMeans clustering on sample data with n_clusters = %d" % n_clusters, fontsize=14, fontweight="bold")

plt.show()

"""Based on Silhouette analysis, the cluster with n = 2 has the highest Silhouette score at 0.48, indicating a relatively good separation of data points within clusters. However, when examining the plot, the cluster with n = 3 appears to be even more distinctly separated, despite having a slightly lower Silhouette score of 0.42."""



"""## Business Recommendation"""

df_result = pd.DataFrame(data=df_cluster,columns=df_cluster.columns)
df_result['Cluster'] = kmeans_3.labels_
df_result.reset_index().head(3)

df_result.shape

df_result ['CustomerID'] = df_merged2['CustomerID']
df_result = df_result.groupby("Cluster").agg({"CustomerID" : "count",
                                              "Qty" : "mean",
                                              "TotalAmount" : "mean"}).reset_index()
df_result.columns = ["Cluster", "Total Customer", "Avg Qty", "Avg TotalAmount"]
df_result



"""Business Recommendation Based on Customer charcteristic cluster :
1. Cluster 0 : Highest total customer, moderate average quantity and total amount
>  Focus on customer retention, implement loyalty programs, personalized recommendations, and excellent customer service to maintain and potentially increase their spending. Consider identifying and offering complementary products to boost their total spending.

2. Cluster 1 : Moderate total customer, lowet average quantity and total amount
> Explore strategies to increase the average transaction value. This may involve bundling products, offering discounts for bulk purchases, or launching targeted promotions to encourage higher spending per transaction.

3. Cluster 2 : Lowest total customer but has highest average quantity and total amount
>  Engage with these high-value customers. Provide premium services, personalized recommendations, and exclusive offers to maintain their loyalty. Continuously monitor their preferences and adapt the product offerings to meet their needs.
"""